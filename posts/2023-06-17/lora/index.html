<!doctype html><html lang=en-us data-theme=dark><head><meta charset=utf-8><meta name=HandheldFriendly content="True"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=referrer content="no-referrer-when-downgrade"><title>LoRA finetuning - Victor's Blog</title><meta name=description content="LoRAs are a recent approach in finetuning LLM and Diffusion models.
They work by injecting new weights to specific layers,
usually at the attention layers as they are some of the most crutial parts of these models.
They allow both efficient fine tuning and lower file size for sharing fine tuned models.
Diffusion models fine tuning exists in many forms, the most frequent are Dreambooth, textual inversion and LoRA. The latter is the most common form as they allow for an easy and fast finetuning and is compatible with the Dreambooth approach."><link rel=icon type=image/x-icon href=https://guichardvictor.github.io/favicon.ico><link rel=apple-touch-icon-precomposed href=https://guichardvictor.github.io/favicon.png><style>body{visibility:hidden;opacity:0}</style><noscript><style>body{visibility:visible;opacity:1}</style></noscript><link rel=stylesheet href=https://guichardvictor.github.io/css/style.min.4254a830200ec9144beafb7a255dc8c989269e2c557baff857b9bbcc94219b1a.css integrity="sha256-QlSoMCAOyRRL6vt6JV3IyYkmnixVe6/4V7m7zJQhmxo="><script src=https://guichardvictor.github.io/js/script.min.74bf1a3fcf1af396efa4acf3e660e876b61a2153ab9cbe1893ac24ea6d4f94ee.js type=text/javascript integrity="sha256-dL8aP88a85bvpKzz5mDodrYaIVOrnL4Yk6wk6m1PlO4="></script><meta property="og:title" content="LoRA finetuning"><meta property="og:description" content="LoRAs are a recent approach in finetuning LLM and Diffusion models.
They work by injecting new weights to specific layers,
usually at the attention layers as they are some of the most crutial parts of these models.
They allow both efficient fine tuning and lower file size for sharing fine tuned models.
Diffusion models fine tuning exists in many forms, the most frequent are Dreambooth, textual inversion and LoRA. The latter is the most common form as they allow for an easy and fast finetuning and is compatible with the Dreambooth approach."><meta property="og:type" content="article"><meta property="og:url" content="https://guichardvictor.github.io/posts/2023-06-17/lora/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-02-03T16:24:50+01:00"><meta property="article:modified_time" content="2023-02-03T16:24:50+01:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="LoRA finetuning"><meta name=twitter:description content="LoRAs are a recent approach in finetuning LLM and Diffusion models.
They work by injecting new weights to specific layers,
usually at the attention layers as they are some of the most crutial parts of these models.
They allow both efficient fine tuning and lower file size for sharing fine tuned models.
Diffusion models fine tuning exists in many forms, the most frequent are Dreambooth, textual inversion and LoRA. The latter is the most common form as they allow for an easy and fast finetuning and is compatible with the Dreambooth approach."><link rel=webmention href=https://webmention.io/hugo-theme-anubis/webmention><link rel=pingback href=https://webmention.io/hugo-theme-anubis/xmlrpc><link rel=webmention href=https://yourdomain.com/webemntions/receive></head><body><a class=skip-main href=#main>Skip to main content</a><div class=container><header class=common-header><div class=header-top><h1 class=site-title><a href=/>Victor's Blog</a></h1><ul class=social-icons><li><a href=https://github.com/GuichardVictor title=Github rel=me><span class=inline-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></a></li><li><a href=https://www.linkedin.com/in/victor-guichard title=Linkedin rel=me><span class=inline-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentcolor" d="M416 32H31.9C14.3 32 0 46.5.0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6.0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3.0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2.0 38.5 17.3 38.5 38.5.0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6.0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2.0 79.7 44.3 79.7 101.9V416z"/></svg></span></a></li><li><a href=https://guichardvictor.github.io/index.xml title=RSS rel=me><span class=inline-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentcolor" d="M128.081 415.959c0 35.369-28.672 64.041-64.041 64.041S0 451.328.0 415.959s28.672-64.041 64.041-64.041 64.04 28.673 64.04 64.041zm175.66 47.25c-8.354-154.6-132.185-278.587-286.95-286.95C7.656 175.765.0 183.105.0 192.253v48.069c0 8.415 6.49 15.472 14.887 16.018 111.832 7.284 201.473 96.702 208.772 208.772.547 8.397 7.604 14.887 16.018 14.887h48.069c9.149.001 16.489-7.655 15.995-16.79zm144.249.288C439.596 229.677 251.465 40.445 16.503 32.01 7.473 31.686.0 38.981.0 48.016v48.068c0 8.625 6.835 15.645 15.453 15.999 191.179 7.839 344.627 161.316 352.465 352.465.353 8.618 7.373 15.453 15.999 15.453h48.068c9.034-.001 16.329-7.474 16.005-16.504z"/></svg></span></a></li></ul></div><nav><a href=https://guichardvictor.github.io/about/ title=About>About</a>
<a href=https://guichardvictor.github.io/posts/ title=Archive>Archive</a>
<a href=https://guichardvictor.github.io/categories/ title=Categories>Categories</a>
<a href=https://guichardvictor.github.io/tags/ title=Tags>Tags</a></nav></header><main id=main tabindex=-1><article class="post h-entry"><div class=post-header><header><h1 class="p-name post-title">LoRA finetuning</h1></header></div><div class="content e-content"><p>LoRAs are a recent approach in finetuning LLM and Diffusion models.
They work by injecting new weights to specific layers,
usually at the attention layers as they are some of the most crutial parts of these models.</p><p>They allow both efficient fine tuning and lower file size for sharing fine tuned models.</p><p>Diffusion models fine tuning exists in many forms, the most frequent are Dreambooth, textual inversion and LoRA. The latter is the most common form as they allow for an easy and fast finetuning and is compatible with the Dreambooth approach.</p><h2 id=lora-dreambooth>LoRA Dreambooth
<span><a href=#lora-dreambooth><svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 007.54.54l3-3a5 5 0 00-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/><path d="M14 11a5 5 0 00-7.54-.54l-3 3a5 5 0 007.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/></svg></a></span></h2><p><ins>@cloneofsimo</ins> was the first to introduce this concept to Stable Diffusion. In this blog, I will use his <a href=https://github.com/cloneofsimo/lora>repository</a> to showcase an example of training and inference a new object concept.</p><h3 id=training>Training
<span><a href=#training><svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 007.54.54l3-3a5 5 0 00-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/><path d="M14 11a5 5 0 00-7.54-.54l-3 3a5 5 0 007.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/></svg></a></span></h3><p>While training a diffusion model can be costly, training a LoRA is very efficient as it only requires to train the LoRA weights while freezing the others.</p><p>LoRA Dreambooth allow to learn new concept in both the object and style form. We will focus on the object form, but the training and inference approach is exactly the saame for the style form.</p><p>Using Huggingface efforts to accelerate and simplify training we can easily launch the process with a single command and without the need of a extremly powerful gpu.</p><p>This is how you&rsquo;d use it to fine-tune a model using pictures of <strong>my dog</strong>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>export MODEL_NAME<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;stabilityai/stable-diffusion-2-1-base&#34;</span>
</span></span><span style=display:flex><span>export INSTANCE_DIR<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;./data/mug&#34;</span>
</span></span><span style=display:flex><span>export OUTPUT_DIR<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;./trained_models/&#34;</span>
</span></span><span style=display:flex><span>export TEMPLATE<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;object&#34;</span>
</span></span><span style=display:flex><span>export TOKEN<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;&lt;mydog&gt;&#34;</span>
</span></span><span style=display:flex><span>export DEVICE<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;cuda:0&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>lora_pti <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --pretrained_model_name_or_path<span style=color:#f92672>=</span>$MODEL_NAME  <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --instance_data_dir<span style=color:#f92672>=</span>$INSTANCE_DIR <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --output_dir<span style=color:#f92672>=</span>$OUTPUT_DIR <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --train_text_encoder <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --resolution<span style=color:#f92672>=</span><span style=color:#ae81ff>512</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --train_batch_size<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --gradient_accumulation_steps<span style=color:#f92672>=</span><span style=color:#ae81ff>4</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --scale_lr <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --learning_rate_unet<span style=color:#f92672>=</span>1e-4 <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --learning_rate_text<span style=color:#f92672>=</span>1e-5 <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --learning_rate_ti<span style=color:#f92672>=</span>5e-4 <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --color_jitter <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --lr_scheduler<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;linear&#34;</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --lr_warmup_steps<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --placeholder_tokens<span style=color:#f92672>=</span>$TOKEN <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --use_template<span style=color:#f92672>=</span>$TEMPLATE <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --save_steps<span style=color:#f92672>=</span><span style=color:#ae81ff>100</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --max_train_steps_ti<span style=color:#f92672>=</span><span style=color:#ae81ff>1000</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --max_train_steps_tuning<span style=color:#f92672>=</span><span style=color:#ae81ff>1000</span> <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --perform_inversion<span style=color:#f92672>=</span>True <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --clip_ti_decay <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --weight_decay_ti<span style=color:#f92672>=</span>0.000 <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --weight_decay_lora<span style=color:#f92672>=</span>0.001<span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --continue_inversion <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --continue_inversion_lr<span style=color:#f92672>=</span>1e-4 <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --device<span style=color:#f92672>=</span>$DEVICE <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>  --lora_rank<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span> <span style=color:#ae81ff>\
</span></span></span></code></pre></div><p>Running this using on an rtx 2080 ti should take around 15 minutes and will generate multiple checkpoint of the trained LoRA.</p><h3 id=inference>Inference
<span><a href=#inference><svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 007.54.54l3-3a5 5 0 00-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/><path d="M14 11a5 5 0 00-7.54-.54l-3 3a5 5 0 007.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/></svg></a></span></h3><p>To infer with trained LoRA, we need to patch the stable diffusion model used. Basically, we load the LoRA weights, and add them to the required layers. We can then scale the LoRA weights using an alpha parameter that will follow this formula:</p><p>$W_{sd} = W_{StableDiffusion} + \alpha W_{LoRA} $</p><p>By setting $\alpha$ at 0, we will only use the weights of Stable Diffusion, setting at 1 will make it fully use the LoRA weights, and greater than 1 will make the model mainly use the LoRA weights.</p><p>The inference code is as simple as this:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=display:flex><span><span style=color:#f92672>from</span> diffusers <span style=color:#f92672>import</span> StableDiffusionPipeline, EulerAncestralDiscreteScheduler
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> lora_diffusion <span style=color:#f92672>import</span> tune_lora_scale, patch_pipe
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> argparse
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>parse_args</span>():
</span></span><span style=display:flex><span>    parser <span style=color:#f92672>=</span> argparse<span style=color:#f92672>.</span>ArgumentParser()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    parser<span style=color:#f92672>.</span>add_argument(<span style=color:#e6db74>&#34;--model&#34;</span>)
</span></span><span style=display:flex><span>    parser<span style=color:#f92672>.</span>add_argument(<span style=color:#e6db74>&#34;--prompt&#34;</span>)
</span></span><span style=display:flex><span>    parser<span style=color:#f92672>.</span>add_argument(<span style=color:#e6db74>&#34;--scale&#34;</span>, type<span style=color:#f92672>=</span>float, default<span style=color:#f92672>=</span><span style=color:#ae81ff>0.8</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    args <span style=color:#f92672>=</span> parser<span style=color:#f92672>.</span>parse_args()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> args
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> __name__ <span style=color:#f92672>==</span> <span style=color:#e6db74>&#39;__main__&#39;</span>:
</span></span><span style=display:flex><span>    args <span style=color:#f92672>=</span> parse_args()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Load Stable Diffusion</span>
</span></span><span style=display:flex><span>    model_id <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;stabilityai/stable-diffusion-2-1-base&#34;</span>
</span></span><span style=display:flex><span>    pipe <span style=color:#f92672>=</span> StableDiffusionPipeline<span style=color:#f92672>.</span>from_pretrained(
</span></span><span style=display:flex><span>        model_id,
</span></span><span style=display:flex><span>        torch_dtype<span style=color:#f92672>=</span>torch<span style=color:#f92672>.</span>float16
</span></span><span style=display:flex><span>    )<span style=color:#f92672>.</span>to(<span style=color:#e6db74>&#34;cuda&#34;</span>)
</span></span><span style=display:flex><span>    pipe<span style=color:#f92672>.</span>scheduler <span style=color:#f92672>=</span> EulerAncestralDiscreteScheduler<span style=color:#f92672>.</span>from_config(
</span></span><span style=display:flex><span>        pipe<span style=color:#f92672>.</span>scheduler<span style=color:#f92672>.</span>config
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Apply the LoRA layers on the model</span>
</span></span><span style=display:flex><span>    patch_pipe(
</span></span><span style=display:flex><span>        pipe,
</span></span><span style=display:flex><span>        args<span style=color:#f92672>.</span>model,
</span></span><span style=display:flex><span>        patch_text<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>        patch_ti<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>        patch_unet<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Tune the LoRA alpha</span>
</span></span><span style=display:flex><span>    tune_lora_scale(pipe<span style=color:#f92672>.</span>unet, args<span style=color:#f92672>.</span>scale)
</span></span><span style=display:flex><span>    tune_lora_scale(pipe<span style=color:#f92672>.</span>text_encoder, args<span style=color:#f92672>.</span>scale)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Inference</span>
</span></span><span style=display:flex><span>    torch<span style=color:#f92672>.</span>manual_seed(<span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>    image <span style=color:#f92672>=</span> pipe(
</span></span><span style=display:flex><span>        args<span style=color:#f92672>.</span>prompt,
</span></span><span style=display:flex><span>        num_inference_steps<span style=color:#f92672>=</span><span style=color:#ae81ff>50</span>,
</span></span><span style=display:flex><span>        guidance_scale<span style=color:#f92672>=</span><span style=color:#ae81ff>7</span>
</span></span><span style=display:flex><span>    )<span style=color:#f92672>.</span>images[<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>    image<span style=color:#f92672>.</span>save(<span style=color:#e6db74>&#34;output.jpg&#34;</span>)
</span></span></code></pre></div><p>Here is an image of my cute dog:</p><p align=center><img src=images/my_dog.png></br><em>My cute dog :D</em></p><p>And here are some generated images:</p><p align=center><img src=images/generated_1.png>
<img src=images/generated_2.png></br><em>< dog > sitting at the beach</em></p><h3 id=side-notes>Side Notes
<span><a href=#side-notes><svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg"><path d="M10 13a5 5 0 007.54.54l3-3a5 5 0 00-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/><path d="M14 11a5 5 0 00-7.54-.54l-3 3a5 5 0 007.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/></svg></a></span></h3><p>A recent paper of Google claims to provide much better results in the styling category using approach called (<strong>StyleDrop</strong>)[https://styledrop.github.io/].
Combining <strong>StyleDrop</strong> for the style and <strong>Dreambooth</strong> to learn new objects might allow to generate images with even more accurate results for more specific use cases.</p></div><div class=post-info><div class="post-date dt-published">2023-02-03</div><a class="post-hidden-url u-url" href=https://guichardvictor.github.io/posts/2023-06-17/lora/>https://guichardvictor.github.io/posts/2023-06-17/lora/</a>
<a href=https://guichardvictor.github.io class="p-name p-author post-hidden-author h-card" rel=me>Victor Guichard</a><div class=post-taxonomies><ul class=post-categories><li><a href=https://guichardvictor.github.io/categories/programming/>programming</a></li></ul><ul class=post-tags><li><a href=https://guichardvictor.github.io/tags/ml/>#ml</a></li><li><a href=https://guichardvictor.github.io/tags/research/>#research</a></li></ul></div></div></article><h3 class=read-next-title>Read next</h3><ul class=read-next-posts><li><a href=/posts/2023-02-03/triton-rs/>Rust Triton Client</a></li></ul><div class="pagination post-pagination"><div class="left pagination-item disabled"></div><div class="right pagination-item"><a href=/posts/2023-02-03/triton-rs/>Rust Triton Client</a></div></div></main><footer class=common-footer><div class=common-footer-bottom><div class=copyright><p>© Victor Guichard, 2023<br>Powered by <a target=_blank rel="noopener noreferrer" href=https://gohugo.io/>Hugo</a>, theme <a target=_blank rel="noopener noreferrer" href=https://github.com/mitrichius/hugo-theme-anubis>Anubis</a>.<br></p></div><script>const STORAGE_KEY="user-color-scheme",defaultTheme="dark-without-switcher";let currentTheme,switchButton,autoDefinedScheme=window.matchMedia("(prefers-color-scheme: dark)");const autoChangeScheme=e=>{currentTheme=e.matches?"dark":"light",document.documentElement.setAttribute("data-theme",currentTheme),changeButtonText()};document.addEventListener("DOMContentLoaded",function(){switchButton=document.querySelector(".theme-switcher"),currentTheme=detectCurrentScheme(),currentTheme=="dark"&&document.documentElement.setAttribute("data-theme","dark"),currentTheme=="auto"&&(autoChangeScheme(autoDefinedScheme),autoDefinedScheme.addListener(autoChangeScheme)),switchButton&&(changeButtonText(),switchButton.addEventListener("click",switchTheme,!1)),showContent()});function detectCurrentScheme(){return localStorage.getItem(STORAGE_KEY)?localStorage.getItem(STORAGE_KEY):defaultTheme?defaultTheme:window.matchMedia?window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light":"light"}function changeButtonText(){switchButton&&(switchButton.textContent=currentTheme=="dark"?"Light theme":"Dark theme")}function switchTheme(){currentTheme=="dark"?(localStorage.setItem(STORAGE_KEY,"light"),document.documentElement.setAttribute("data-theme","light"),currentTheme="light"):(localStorage.setItem(STORAGE_KEY,"dark"),document.documentElement.setAttribute("data-theme","dark"),currentTheme="dark"),changeButtonText()}function showContent(){document.body.style.visibility="visible",document.body.style.opacity=1}</script></div><p class="h-card vcard"><a href=https://guichardvictor.github.io class="p-name u-url url fn" rel=me>Victor Guichard</a>
/
<a class="p-email u-email email" rel=me href=mailto:guichardvictor@gmail.com>guichardvictor@gmail.com</a>
<img class=u-photo src=/images/me.png></p></footer></div></body></html>